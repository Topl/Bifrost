package co.topl.it

import cats.implicits._
import co.topl.attestation.Address
import co.topl.it.util._
import co.topl.rpc.ToplRpc
import com.typesafe.config.ConfigFactory
import io.circe.syntax._
import org.scalatest.Inspectors
import org.scalatest.concurrent.PatienceConfiguration.Timeout
import org.scalatest.freespec.AnyFreeSpec
import org.scalatest.matchers.should.Matchers

import scala.concurrent.Future
import scala.concurrent.duration._

class MultiNodeTest extends AnyFreeSpec with Matchers with IntegrationSuite with Inspectors {

  val nodeCount: Int = 3
  // Values close to 0.0 indicate a strict/tight fairness threshold
  val blockDistributionTolerance: Double = 0.99d
  val forgeDuration: FiniteDuration = 5.minutes
  val seed: String = "MultiNodeTest" + System.currentTimeMillis()

  "Multiple nodes can forge blocks with roughly equal distribution" in {

    val nodes = createAndStartNodes()

    assignForgingAddresses(nodes)

    // Fetch the initial count of blocks generated by address.  Because forging has not started, only a single
    // genesis block should be assigned to a single address
    val initialGeneratorCounts: Map[String, Map[Address, Int]] =
      nodes
        .map(node =>
          node.containerId -> node.run(ToplRpc.Debug.Generators.rpc)(ToplRpc.Debug.Generators.Params()).value
        )
        .toMap

    initialGeneratorCounts.foreach { case (containerId, generatorCounts) =>
      logger.info(
        s"Initial block generator counts for containerId=$containerId:\n${generatorCounts.asJson.spaces2SortKeys}"
      )
    }

    forAll(initialGeneratorCounts.values) { counts =>
      counts should have size 1
      counts.head._2 shouldBe 1L
      forAll(initialGeneratorCounts.values)(counts should contain theSameElementsAs _)
    }

    val genesisAddress = initialGeneratorCounts.head._2.head._1

    // Now instruct the nodes to start forging
    nodes.foreach(_.run(ToplRpc.Admin.StartForging.rpc)(ToplRpc.Admin.StartForging.Params()).value)

    // wait for forging and periodically query some data about the nodes
    logger.info(s"Waiting $forgeDuration for forging")
    val endTime = System.currentTimeMillis() + forgeDuration.toMillis
    while (System.currentTimeMillis() < endTime) {
      val intermediateResult = nodes.map { node =>
        val headInfo = node.run(ToplRpc.NodeView.HeadInfo.rpc)(ToplRpc.NodeView.HeadInfo.Params()).value
        val openKeyfile = node.run(ToplRpc.Admin.ListOpenKeyfiles.rpc)(ToplRpc.Admin.ListOpenKeyfiles.Params()).value
        val localBlockView = node.run(ToplRpc.Debug.Generators.rpc)(ToplRpc.Debug.Generators.Params()).value

        s"\nFor ${node.containerId}: \n\t keyfiles: ${openKeyfile} \n\t headInfo: ${headInfo} \n\t localBlockView: ${localBlockView}"
      }

      logger.info(s"\n$intermediateResult\n")

      Thread.sleep(forgeDuration.toMillis / 10)
    }

    // Now instruct the nodes to stop forging
    nodes.foreach(_.run(ToplRpc.Admin.StopForging.rpc)(ToplRpc.Admin.StopForging.Params()).value)

    // Verify that each node has forged a roughly equal number of blocks according to their own "myBlocks" information
    val forgeCounts =
      nodes
        .map(node =>
          node.containerId -> node.run(ToplRpc.Debug.MyBlocks.rpc)(ToplRpc.Debug.MyBlocks.Params()).value.count
        )
        .toMap

    forgeCounts.foreach { case (containerId, count) =>
      logger.info(s"myBlocks forging count=$count containerId=$containerId")
    }

    forAll(forgeCounts.values)(_ should be > 0)

    val mean = forgeCounts.values.sum / forgeCounts.size

    forAll(forgeCounts.values)(_ should be(mean +- (mean * blockDistributionTolerance).toInt))

    // And now verify that the nodes have shared understanding of how many blocks their peers have forged
    val finalGeneratorCounts: Map[String, Map[Address, Int]] =
      nodes
        .map(node =>
          node.containerId -> node.run(ToplRpc.Debug.Generators.rpc)(ToplRpc.Debug.Generators.Params()).value
        )
        .toMap

    finalGeneratorCounts.foreach { case (containerId, generatorCounts) =>
      logger.info(
        s"Final block generator counts for containerId=$containerId:\n${generatorCounts.asJson.spaces2SortKeys}"
      )
    }

    forAll(finalGeneratorCounts.values) { counts =>
      counts should have size (nodeCount + 1)
      counts(genesisAddress) shouldBe 1L
      forAll(finalGeneratorCounts.values)(counts should contain theSameElementsAs _)
    }

    val headGeneratorCounts =
      finalGeneratorCounts.head._2 - genesisAddress

    headGeneratorCounts should have size nodes.size

    // And verify again that the block distribution was fair
    val generatorCountMean = headGeneratorCounts.values.sum / headGeneratorCounts.size

    forAll(headGeneratorCounts.values)(_ should be(mean +- (generatorCountMean * blockDistributionTolerance).toInt))
  }

  /**
   * Launches a group of nodes, all on the same Docker network and sharing the same seed.  Forging
   * is disabled on startup.
   */
  def createAndStartNodes(): List[BifrostDockerNode] = {
    val nodeNames = List.tabulate(nodeCount)("bifrostMultiNode" + _)

    val config =
      ConfigFactory.parseString(
        raw"""bifrost.network.knownPeers = ${nodeNames.map(n => s"$n:${BifrostDockerNode.NetworkPort}").asJson}
             |bifrost.rpcApi.namespaceSelector.debug = true
             |bifrost.forging.addressGenerationSettings.numberOfAddresses = $nodeCount
             |bifrost.forging.addressGenerationSettings.strategy = fromSeed
             |bifrost.forging.addressGenerationSettings.addressSeedOpt = "$seed"
             |bifrost.forging.forgeOnStartup = false
             |""".stripMargin
      )

    val nodes = nodeNames.map(dockerSupport.createNode(_, "MultiNodeTest"))

    nodes.foreach(_.reconfigure(config))
    nodes.foreach(_.start())

    Thread.sleep(20.seconds.toMillis)

    // Startup may take a bit longer in multi-node tests because they need to synchronize first
    Future
      .traverse(nodes)(_.waitForStartup().map(_.value))
      .futureValue(Timeout(60.seconds))

    nodes
  }

}
